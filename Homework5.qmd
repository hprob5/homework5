---
title: "Homework5"
author: "Holly Probasco"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

### 1. What is the purpose of using cross-validation when fitting a random forest model?

-   The purpose is to help choose the best model within the random forest selection process. It shows the error for the models used and also the accuracy of the predictions.

### 2. Describe the bagged tree algorithm.

-   The process involves creating many bootstrap resamples. Each resample creates a new tree. Then, the predictions from each tree are combined in some way to then create the final prediction. For regression trees this is taking the mean, and for classification trees the most common prediction is used.

### 3. What is meant by a general linear model?

-   A general linear model is a linear model that is able to be used on multiple predictors, which includes categorical variables and also interaction terms.

### 4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

-   Interaction terms allow one predictor to be dependant on another, allowing for more model flexibility. Otherwise, each of the terms in the model are indepedent of one another.

### 5. Why do we split our data into a training and test set?

-   We do this so that after training our model, we can test it on data it hasn't seen. Otherwise, we are training our model just to the specific data we have and have no way of knowing it will be accurate on new data.

## Task 2: Data Prep
```{r, message=FALSE}
lapply(c("tidyverse", "tidymodels", "caret", "yardstick"), library, character.only = TRUE)
```

```{r}
heartdata <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/heart.csv") %>% 
  as.tibble()
```


### 1. Run and report summary() on your data set.
```{r}
summary(heartdata)
```


#### a. What type of variable is Heart Disease?
Quantitative

#### b. Does this make sense? 
No, this variable is meant to be a binary Y/N, for whether or not Heart Disease is present. This would be categorical.

### 2. Change HeartDisease to be the appropriate data type, and name it something different. In the same tidyverse pipeline, remove the ST_Slope variable and the original HeartDisease variable. Save your new data set as new_heart.
```{r}
new_heart <- heartdata %>% mutate(DiseasePresent = as.factor(HeartDisease), 
HeartDisease = NULL, ST_Slope = NULL)
```

## Task 3: EDA
### 1. 
Create the appropriate scatterplot to visualize this relationship (age as a function of heart disease and max heart rate)
```{r,message=FALSE}
ggplot(new_heart, aes(x = MaxHR, y = Age, color = DiseasePresent)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) + 
labs(x = "Max Heartrate", y= "Age") +
scale_color_manual(values = c("0" = "#0072B2", "1" = "#E69F00"))
#colors taken from the cbPalette (colorblind friendly Palette) used on the cookbook-R site
```

### 2. Based on visual evidence, do you think an interaction model or an additive model is more appropriate? Justify your answer.
- Looking at the graph, I think that an interaction model is more appropriate, since the lines for DiseasePresent cross. This could indicate that there is a dependence within the variables of the model. We should investigate to see the if impact of MaxHR on Age changes if Disease is Present.

## Task 4: Testing and Training
### 1. Split your data into a training and test set.
```{r}
set.seed(101)
split_data <- initial_split(new_heart, prop = 0.8)
train <- training(split_data)
test <- testing(split_data)
```


## Task 5: OLS and LASSO
### 1. Fit an interaction model (named ols_mlr) with age as your response, and max heart rate + heart disease as your explanatory variables using the training data set using ordinary least squares regression. 
```{r}
ols_mlr <- lm(Age ~ MaxHR + DiseasePresent, data = train)

#Report the summary output.
summary(ols_mlr)
```

### 2. Use RMSE to evaluate this model’s predictive performance on new data. Test your model on the testing data set. Calculate the residual mean square error (RMSE) and report it below.
```{r}
yardstick::rmse_vec(test$Age, predict(ols_mlr, newdata = test))
```
RMSE = 9.01

### 3. See if a model fit using LASSO has better predictive performance than with OLS. Use cross validation to select the best tuning parameter, then evaluate our LASSO model on the testing data set and compare RMSEs.

10 fold cv 
```{r}
heart_CV_folds <- vfold_cv(train, 10)
```

```{r}
LASSO_recipe <- recipe(Age ~ MaxHR + DiseasePresent, data = train) |>
  step_dummy(DiseasePresent) |>              
  step_normalize(all_numeric_predictors()) |>
  step_interact(~MaxHR:starts_with("DiseasePresent"))


LASSO_recipe
```

### 4.
set up spec
```{r}
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

#add to workflow
LASSO_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)
LASSO_wkf
```

```{r}
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_CV_folds,
         grid = grid_regular(penalty(), levels = 200)) 

#collect the best metric using RMSE
best_met <- LASSO_grid |> select_best(metric = "rmse")
best_met
```
now we can finalize our workflow by using the lowest RMSE we found to make the best LASSO model
```{r}
LASSO_final <- LASSO_wkf |>
  finalize_workflow(best_met) |>
  fit(train)
tidy(LASSO_final)
```


### 5. Without looking at the RMSE calculations, would you expect the RMSE calculations to be roughly the same or different? Justify your answer using output from your LASSO model
I would expect them to be roughly the same because the tuning parameter we found to be best for LASSO is so small


### 6. Compare the RMSE between your OLS and LASSO model and show that the RMSE calculations were roughly the same.
```{r}
LASSO_final |>
  predict(test) |>
  pull() |>
  rmse_vec(truth = test$Age)
```
Though slightly larger, RMSE is also about 9.01 for LASSO

### 7. Why are the RMSE calculations roughly the same if the coeﬀicients for each model are different?
Because RMSE is a measure of the quality of predictions made by the model, it isn't taking coefficients into account

## Task 6: Logistic Regression
Propose two different logistic regression models with heart disease as our response.
```{r}
model1 <- recipe(DiseasePresent ~ MaxHR + ChestPainType + Cholesterol, data = train) |>
step_normalize(all_numeric_predictors())

model2 <- recipe(DiseasePresent ~ MaxHR + Age + RestingECG, data = train) |>
step_normalize(all_numeric_predictors())
```

set up engines for the Log Reg
```{r}
LR_spec <- logistic_reg() |> set_engine("glm")

#then the workflows
LR_wkf1 <- workflow() |>
add_recipe(model1) |>
add_model(LR_spec)

LR_wkf2 <- workflow() |>
add_recipe(model2) |>
add_model(LR_spec)
```

use CV to determine best model of the two
```{r}
LR_fit1 <- LR_wkf1 |>
fit_resamples(heart_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
LR_fit2 <- LR_wkf2 |>
fit_resamples(heart_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
```

see which model did the best by collecting the metrics
```{r}
rbind(LR_fit1 |> collect_metrics(),
LR_fit2 |> collect_metrics()) |>
mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
select(Model, everything())
```
We can see that the first model, with MaxHR, ChestPainType, and Cholesterol, has both higher accuracy in prediction as well as lower log loss, meaning we can have more confidence in it's predictions. This implies that Model1 is the better prediction model for the presence of Heart Disease.

### 2.  check how well your chosen model does on the test set using the confusionMatrix() function
```{r}
#fit best model onto train data
LR_train_fit <- LR_wkf1 |> fit(train)

conf_mat(test |> mutate(estimate = LR_train_fit |> predict(test) |> pull()), 
DiseasePresent, estimate)
```
So we see that the model:
correctly predicted the absence of disease 68 times
correctly predicted the presence of disease 72 times

incorrectly predicted the absence of disease when it was actually there 18 times
incorrectly predicted the presence of disease when there was none 26 times


### 3. Identify the values of sensitivity and specificity, and interpret them in the context of the problem
Sensitivity is correctly predicting the presence of disease when there is actually disease
= 72 / (72 + 18) = 0.8 or 80%
80% of the time the model correctly predicts the presence of disease

Specificity is correctly predicting the absence of disease 
= 68 / (68 + 26) = .723
72.3% of the time the model correctly predicts the absence of disease
